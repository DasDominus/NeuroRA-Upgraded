{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DasDominus/NeuroRA-Upgraded/blob/master/Copy_of_Blindlady_Deep_Learning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZjQVajvaBAP"
      },
      "source": [
        "# Import and Utils Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a5aV479OEFb8"
      },
      "outputs": [],
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OufBFXqIaAN7"
      },
      "outputs": [],
      "source": [
        "# @title install packages if missing\n",
        "%%capture\n",
        "# !pip install nilearn\n",
        "# # token = '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'\n",
        "# !pip install -vvv git+https://{token}@github.com/DasDominus/NostrorumCerebrum@main\n",
        "# # !pip install -vvv git+https://{token}@github.com/DasDominus/nilearn@master\n",
        "# !pip install git+https://github.com/DasDominus/NeuroRA-Upgraded@master#egg=neurora\n",
        "# !pip install pynvml\n",
        "# !pip install torch\n",
        "# !pip install boto3\n",
        "# !pip install bounded_pool_executor\n",
        "# !pip3 freeze > requirements.txt\n",
        "!pip install -r /content/drive/MyDrive/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XCAB9_1_Z6B7"
      },
      "outputs": [],
      "source": [
        "# @title Load Data and Code from Drive\n",
        "%%capture\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "\n",
        "#@markdown Logging\n",
        "LOGGING_LEVEL = logging.INFO #@param {type:\"raw\"}\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(LOGGING_LEVEL)\n",
        "\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = '16'\n",
        "os.environ[\"GPU\"] = \"enabled\"\n",
        "os.chdir(\"/content/drive/MyDrive/Experiment/BrainSurfer\")\n",
        "def AddDirToSys(root='.'):\n",
        "  logging.debug(f'Mounting Python: {root}')\n",
        "  for dir in os.listdir(root):\n",
        "    dir_p = os.path.join(root, dir)\n",
        "    if os.path.isdir(dir_p):\n",
        "      sys.path.append(dir_p)\n",
        "      AddDirToSys(dir_p)\n",
        "\n",
        "roots_to_mount = [\n",
        "  r'/content/drive/MyDrive/Experiment/BrainSurfer/Analytics',\n",
        "  # r'/content/drive/MyDrive/BrainSurfer/neurora',\n",
        "  # r'/content/drive/MyDrive/NeuroRA-Upgraded/neurora',\n",
        "]\n",
        "for root in roots_to_mount:\n",
        "  try:\n",
        "    AddDirToSys(root=root)\n",
        "  except Exception as e:\n",
        "    print (f'Moutning Failed for {root} due to {e}')\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MjEdkZ7jHWIZ"
      },
      "outputs": [],
      "source": [
        "#@markdown Base Imports\n",
        "import time\n",
        "import fnmatch\n",
        "import collections\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from typing import Callable\n",
        "from pathlib import Path\n",
        "from matplotlib import patches, ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import threading\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import nibabel as nb\n",
        "\n",
        "#@markdown Nilearn Modules\n",
        "from nilearn import plotting\n",
        "from nilearn.decomposition import DictLearning\n",
        "from nilearn.masking import apply_mask\n",
        "from nilearn.image import get_data, mean_img, index_img, load_img, iter_img\n",
        "from nilearn.regions import Parcellations, RegionExtractor\n",
        "from nilearn.interfaces.fmriprep import load_confounds\n",
        "from nilearn.plotting import plot_prob_atlas, plot_stat_map, show\n",
        "\n",
        "#@markdown Analytics Modules\n",
        "from Analytics.configs import env_vars\n",
        "from Analytics.utils.data_manager import manager as dm\n",
        "from Analytics.utils.file_ops import file_utils\n",
        "from Analytics.SimilarityAnalysis import correlation_measures\n",
        "from Analytics.common import models\n",
        "\n",
        "import helper_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2XEYMGgbL69"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import aws_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZMdiOV7TorG"
      },
      "outputs": [],
      "source": [
        "\n",
        "target_directory = 'Results/exp_bl/temporal/community' #@param {type: \"string\"}\n",
        "download_dir = '/content/drive/MyDrive/Experiment/AWS_RESULTS' #@param {type: \"string\"}\n",
        "dry_run = True #@param {type: \"boolean\"}\n",
        "\n",
        "file_filter = None #@param {type: \"raw\"}\n",
        "aws_mgr = aws_utils.AWSManager(bucket='brainsurfer-fin')\n",
        "aws_mgr.DownloadAllFilesInFolder(\n",
        "    folder=target_directory,\n",
        "    download_dir=download_dir,\n",
        "    dry_run=dry_run,\n",
        "    file_filter=file_filter,\n",
        "    max_workers=100,\n",
        "    max_queue=500\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe0MLpE-FpwV"
      },
      "source": [
        "# Atlas Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bAo-0nYPy7E1"
      },
      "outputs": [],
      "source": [
        "use_aicha = True #@param {type: \"boolean\"}\n",
        "use_harvard = False #@param {type: \"boolean\"}\n",
        "use_canica = True #@param {type: \"boolean\"}\n",
        "use_ward = False #@param {type: \"boolean\"}\n",
        "use_kmeans = True #@param {type: \"boolean\"}\n",
        "\n",
        "atlases = []\n",
        "if use_aicha:\n",
        "  atlases.append('aicha')\n",
        "  \n",
        "if use_harvard:\n",
        "  atlases.append('harvard')\n",
        "  \n",
        "if use_canica:\n",
        "  atlases.append('canica')\n",
        "  \n",
        "if use_ward:\n",
        "  atlases.append('ward')\n",
        "  \n",
        "if use_kmeans:\n",
        "  atlases.append('kmeans')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLu6AqiNpMVl"
      },
      "outputs": [],
      "source": [
        "methods = [\n",
        "  # 'raw',\n",
        "  'avg',\n",
        "  'pct',\n",
        "  'pca',\n",
        "  'pca_time'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtH2rvZgy21T"
      },
      "source": [
        "# 0. Experiment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AO3je2ky_T1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown SetGlobalParams\n",
        "dataset = {}\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "#@markdown SetParams\n",
        "root = os.getcwd()\n",
        "_DATA_ROOT = '/Data'\n",
        "_EXP_ROOT = f'{root}/Experiment/Processed_Research_Data'\n",
        "_EXP_OUTPUT_DIR = f'{root}/Results'\n",
        "_EXP_CONFIG_NAME = \"colab_bl\" #@param{type: \"string\"}\n",
        "_EXP_LABEL = 'exp_' + _EXP_CONFIG_NAME.split('_')[1]\n",
        "_PARCELLATION_RESULT_ROOT = f\"{_EXP_OUTPUT_DIR}/{_EXP_LABEL}/parcellation\"\n",
        "logging.info(f'_PARCELLATION_RESULT_ROOT: {_PARCELLATION_RESULT_ROOT}')\n",
        "\n",
        "_EXP_CONFIG = env_vars.STUDY_ENV_TO_CONFIG[_EXP_CONFIG_NAME]\n",
        "_EXP_CONFIG_FILE = str(\n",
        "    _EXP_CONFIG.config_root / _EXP_CONFIG.config_file)\n",
        "logging.info(f'_EXP_CONFIG_FILE set to: {_EXP_CONFIG_FILE}')\n",
        "_EXP_DATA_ROOT = os.getcwd() / _EXP_CONFIG.kwargs.get(env_vars.SupportedKwargs.DATA_ROOT)\n",
        "logging.info(\n",
        "    f'DATA_ROOT \\n\\t_EXP_DATA_ROOT: {_EXP_DATA_ROOT} \\n\\t S3: {1 }')\n",
        "\n",
        "#@markdown Set Up Dir\n",
        "file_utils.InitDir(_PARCELLATION_RESULT_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0o-ZH0A_zGtr"
      },
      "outputs": [],
      "source": [
        "#@title Atlas Mapping\n",
        "import nibabel as nb\n",
        "\n",
        "_ENV_ROOT = \"/home/ubuntu\" #@param {type: \"string\"}\n",
        "_DATA_MOUNT= \"Data\" #@param {type: \"string\"}\n",
        "#@markdown Load Functional Atlas\n",
        "class Atlas:\n",
        "  harvard_atlas = (\n",
        "      'harvard',\n",
        "      f'{_ENV_ROOT}/Data/Experiment/Processed_Research_Data/atlas/atlas_harvard/atlas.nii')             \n",
        "  aicha_atlas = (\n",
        "      'aicha',\n",
        "      f'{_ENV_ROOT}/Data/Experiment/Processed_Research_Data/atlas/atlas_aicha/AICHA.nii')\n",
        "  ward_atlas = (\n",
        "      'ward',\n",
        "      f'{_ENV_ROOT}/Results/{_EXP_LABEL}/parcellation_valid/ward/ward_parcellation.nii.gz'\n",
        "  )\n",
        "  kmeans_atlas = (\n",
        "      'kmeans',\n",
        "      f'{_ENV_ROOT}/Results/{_EXP_LABEL}/parcellation_valid/kmeans/kmeans_parcellation.nii.gz'\n",
        "  )\n",
        "  canica_atlas = (\n",
        "      'canica',\n",
        "      f'{_ENV_ROOT}/Results/{_EXP_LABEL}/parcellation_valid/CanICA/canica_resting_state.nii.gz'\n",
        "  )\n",
        "atlas_data = Atlas()\n",
        "\n",
        "def GetAtlasLabels(atlas_fn: str):\n",
        "  atlas_img = nb.load(atlas_fn)\n",
        "  logging.info(atlas_img.shape)\n",
        "  labels = np.unique(atlas_img.dataobj)\n",
        "  return [str(i) for i in labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PnP Analysis"
      ],
      "metadata": {
        "id": "vXmdQnpTQvVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "distance_methods = [\n",
        "      'pearson',\n",
        "      'kendall',\n",
        "      'spearman',\n",
        "]\n",
        "\n",
        "\n",
        "sdts_methods = [\n",
        "      ('pearson', ),\n",
        "      ('euclidean', ),\n",
        "      ('mahalanobis', ),\n",
        "      ('cosine', ),\n",
        "]"
      ],
      "metadata": {
        "id": "x-M2oFI2RZqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PnP Functions\n",
        "import json\n",
        "import os.path\n",
        "from typing import Optional, Any\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "class Task:\n",
        "    params = ()\n",
        "    skip = False\n",
        "    key = ''\n",
        "\n",
        "    def __init__(self, params, skip, key):\n",
        "        self.params = params\n",
        "        self.skip = skip\n",
        "        self.key = key\n",
        "\n",
        "import logging\n",
        "from concurrent import futures\n",
        "from tqdm import tqdm\n",
        "\n",
        "def Batches(arr, chunk):\n",
        "    for i in range(0, len(arr), chunk):\n",
        "        yield arr[i:i + chunk]\n",
        "\n",
        "\n",
        "def ExecuteChunkedTasks(\n",
        "        all_tasks, callback, chunk=100, st=0, ed=0, dry_run=False, max_workers=8, max_queue=40, descriptor=''):\n",
        "    logging.debug(f'Processing: {st * chunk}:{ed * chunk}')\n",
        "    trunc_tasks = sorted(all_tasks, key=lambda x: x.key)[st * chunk:ed * chunk]\n",
        "    del all_tasks\n",
        "    proceesed_tasks = [\n",
        "        task for task in trunc_tasks if not task.skip\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "    with tqdm(total=len(proceesed_tasks), desc=\"Processing All Tasks\"+ descriptor, leave=True) as pbar:\n",
        "        with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            for batch in Batches(proceesed_tasks, max_queue):\n",
        "                future_to_key = {}\n",
        "                for task in batch:\n",
        "                    # assume last item in entry is key\n",
        "                    if not dry_run:\n",
        "                        future_to_key[\n",
        "                            executor.submit(\n",
        "                                callback, *task.params\n",
        "                            )] = task.key\n",
        "                logging.debug('Batch Submitted')\n",
        "                for future in futures.as_completed(future_to_key):\n",
        "                    key = future_to_key[future]\n",
        "                    try:\n",
        "                        result = future.result()\n",
        "                        del future_to_key[future]\n",
        "                        results[key] = result\n",
        "                    except Exception as e:\n",
        "                        logging.exception(f'{key} failed due to {e}')\n",
        "                    pbar.update(1)\n",
        "    return results\n",
        "\n",
        "def nested_dict():\n",
        "    return defaultdict(nested_dict)\n",
        "\n",
        "def vGetKendallTau(series, base):\n",
        "    return stats.kendalltau(series, base)\n",
        "\n",
        "_VGKT = np.vectorize(vGetKendallTau)\n",
        "\n",
        "def GetKendalCor(series, k):\n",
        "    \"\"\"\n",
        "    Gets the kendall tau for one time series given interval k.\n",
        "\n",
        "    :param series: time series\n",
        "    :return: calculated correlation coefficients.\n",
        "    \"\"\"\n",
        "    series = list(series)\n",
        "    ss = []\n",
        "    base = [i + 1 for i in range(k)]\n",
        "    for i in range(len(series) - k):\n",
        "        tau, p = stats.kendalltau(series[i:i + k], base)\n",
        "        ss.append((tau, p))\n",
        "    return ss\n",
        "\n",
        "\n",
        "def PnPExtraction(time_series):\n",
        "    results = {}\n",
        "    for phase in range(2, int(len(time_series) / 2)):\n",
        "        tau_series = GetKendalCor(time_series, phase)\n",
        "        # Test if significant\n",
        "        chi_stat, chi_pval = stats.chisquare([t[0] for t in tau_series])\n",
        "        stat = models.TestStat(method='chi', stat=chi_stat, p_value=chi_pval)\n",
        "        results[phase] = stat\n",
        "    return results\n",
        "\n",
        "\n",
        "def PnPDetection(\n",
        "        atlas, method, distance_method, community_base,\n",
        "        data_dir: str,\n",
        "        output_dir: str, plot=True, use_std=False,\n",
        "        sequential_dir: str = None, alpha: float = 0.05, force: bool = False,\n",
        "        dry_run: bool = True\n",
        "):\n",
        "    cs_out_dir = f'{output_dir}/{atlas}/{method}/{community_base}/{distance_method}'\n",
        "    file_utils.InitDir(cs_out_dir)\n",
        "\n",
        "    std_suffix = 'std' if use_std else 'nonstd'\n",
        "\n",
        "    cc_metadata_fn = f'{data_dir}/{atlas}/{method}/{community_base}/metadata.json'\n",
        "    cc_metadata = json.load(open(cc_metadata_fn))\n",
        "\n",
        "    cc_data_dir = sequential_dir or data_dir\n",
        "\n",
        "    out_fn = f'{cs_out_dir}/unfiltered_results.json'\n",
        "    if not force and os.path.exists(out_fn):\n",
        "      odata = json.load(open(out_fn))\n",
        "      if odata:\n",
        "        return\n",
        "\n",
        "    # Calculate cross-sectional correlation\n",
        "    detection_results = nested_dict()\n",
        "    all_tasks = []\n",
        "    for condition in sorted(cc_metadata['conditions']):\n",
        "        for community in cc_metadata['communities']:\n",
        "\n",
        "            if sequential_dir is not None:\n",
        "                cs_data_dir = f'{cc_data_dir}/{atlas}/{method}/{community_base}/{distance_method}/{std_suffix}/{condition}'\n",
        "                dists_fn = f'{cs_data_dir}/{community}_dists.json'\n",
        "                if not os.path.exists(dists_fn):\n",
        "                    continue\n",
        "\n",
        "                dist_series = json.load(open(dists_fn))\n",
        "\n",
        "                for dist_type in dist_series:\n",
        "                    ts = dist_series[dist_type]\n",
        "                    all_tasks.append(Task(\n",
        "                        params=(ts,),\n",
        "                        skip=False,\n",
        "                        key=(condition, community, dist_type)\n",
        "                    ))\n",
        "            else:\n",
        "                cs_data_dir = f'{cc_data_dir}/{atlas}/{method}/{community_base}/{condition}'\n",
        "                signal_fn = f'{cs_data_dir}/'+ f'{community}.json' if not use_std else f'{community}_std.json'\n",
        "\n",
        "                signals = json.load(open(signal_fn))\n",
        "                for roi in cc_metadata['communities'][community]:\n",
        "                    dists = [signals[roi][idx] for idx in sorted(signals[roi], key=lambda x: int(x))]\n",
        "\n",
        "                    all_tasks.append(Task(\n",
        "                        params=(dists,),\n",
        "                        skip=False,\n",
        "                        key=(condition, community, roi)\n",
        "                    ))\n",
        "\n",
        "\n",
        "    results = ExecuteChunkedTasks(\n",
        "        all_tasks=all_tasks,\n",
        "        callback=PnPExtraction,\n",
        "        chunk=100,\n",
        "        st=0,\n",
        "        ed=10,\n",
        "        dry_run=dry_run,\n",
        "        max_workers=30,\n",
        "        max_queue=120,\n",
        "        descriptor=f' {atlas}/{method}/{distance_method}'\n",
        "    )\n",
        "\n",
        "    if not dry_run:\n",
        "      for key, phase_results in results.items():\n",
        "          detection_results[key[0]][key[1]][key[2]] = [\n",
        "              (phase, phase_results[phase].stat, phase_results[phase].p_value) for phase in phase_results\n",
        "          ]\n",
        "      json.dump(detection_results, open(out_fn, 'w+'))"
      ],
      "metadata": {
        "id": "D6O6Ym9SQyWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PnP Detection - 1. Sequential\n",
        "force = False #@param {type: \"boolean\"}\n",
        "dry_run = True  #@param {type: \"boolean\"}\n",
        "def Run(atlases, methods, distance_methods, metadata, community_base):\n",
        "  root = '/content/drive/MyDrive/Experiment/AWS_RESULTS/Results'\n",
        "  for atlas in atlases:\n",
        "    for method in methods:\n",
        "      for distance_method in distance_methods:\n",
        "        try:\n",
        "          PnPDetection(\n",
        "              atlas, method, distance_method[0],\n",
        "              community_base=community_base,\n",
        "              data_dir=f'{root}/{_EXP_LABEL}/temporal/community',\n",
        "              output_dir=f'{root}/{_EXP_LABEL}/temporal/regularity/pnp',\n",
        "              sequential_dir=f'{root}/{_EXP_LABEL}/temporal/sequential',\n",
        "              force=force,\n",
        "              dry_run=dry_run\n",
        "          )\n",
        "        except Exception as e:\n",
        "          print(atlas, method, distance_method[0])\n",
        "          logging.exception(e)\n",
        "\n",
        "Run(atlases, methods, sdts_methods, metadata='metadata.json',community_base='resting')\n",
        "# Run(atlases, methods, sdts_methods, metadata='metadata.json', community_base='mean_task')"
      ],
      "metadata": {
        "id": "sA9pvPAoXdmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PnP Detection - 2. Raw Done in Colab\n",
        "force = False #@param {type: \"boolean\"}\n",
        "dry_run = True  #@param {type: \"boolean\"}\n",
        "def Run(atlases, methods, distance_methods, metadata, community_base):\n",
        "  root = '/content/drive/MyDrive/Experiment/AWS_RESULTS/Results'\n",
        "  for atlas in atlases:\n",
        "    for method in methods:\n",
        "      for distance_method in distance_methods:\n",
        "        try:\n",
        "          PnPDetection(\n",
        "              atlas, method, distance_method[0],\n",
        "              community_base=community_base,\n",
        "              data_dir=f'{root}/{_EXP_LABEL}/temporal/community',\n",
        "              output_dir=f'{root}/{_EXP_LABEL}/temporal/regularity/pnp_raw_signal',\n",
        "              force=force,\n",
        "              dry_run=dry_run\n",
        "          )\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          # logging.exception(e)\n",
        "\n",
        "Run(atlases, methods, sdts_methods, metadata='metadata.json',community_base='resting')\n",
        "# Run(atlases, methods, sdts_methods, metadata='metadata.json', community_base='mean_task')"
      ],
      "metadata": {
        "id": "ezQUHHMdRTBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zraFgtK7U-Ly"
      },
      "source": [
        "# Autoencoder DFC Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8iQdx6OHJzc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Prepare Signals\n",
        "def nested_dict():\n",
        "    return collections.defaultdict(nested_dict)\n",
        "\n",
        "#@markdown Load Signals from File\n",
        "data = nested_dict()\n",
        "for atlas in atlases:\n",
        "  for method in methods:\n",
        "    root = f'/content/drive/MyDrive/Experiment/AWS_RESULTS/Results/{_EXP_LABEL}/signals/{method}/{atlas}/sub-01'\n",
        "    for fn in os.listdir(root):\n",
        "      if not fn.endswith('csv'):\n",
        "        continue\n",
        "      params = fn.split('_roi_')\n",
        "      cond = params[0]\n",
        "      roi = params[-1].split('.')[0]\n",
        "      sig = pd.read_csv(os.path.join(root, fn), index_col=0)\n",
        "\n",
        "      if atlas == 'canica':\n",
        "        roi = str(int(roi)+1)\n",
        "\n",
        "      if method == 'raw':\n",
        "        data[atlas][method][cond][roi] = sig\n",
        "      else:\n",
        "        data[atlas][method][cond][roi] = sig.mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTjDewdHGYlc"
      },
      "outputs": [],
      "source": [
        "#@title Get Network Level Dynamic Correlation Matrices\n",
        "import json\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "skip_cell = False #@param {type: \"boolean\"}\n",
        "force = True #@param {type: \"boolean\"}\n",
        "plot = True #@param {type: \"boolean\"}\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from typing import Any, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nilearn import plotting\n",
        "from nilearn.connectome import ConnectivityMeasure\n",
        "from scipy.stats import zscore\n",
        "from tqdm import tqdm\n",
        "# import cupy\n",
        "\n",
        "def GetBasicCorrelationMatrixForSubjects(\n",
        "        subjects_data: Any, method: str = 'correlation',\n",
        "        output_dir: str = '.', plot_dir: str = '', draw: bool = False,\n",
        "        fn_pattern: str = 'corr_mat-sub_{idx}', **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "\n",
        "    :param subjects_data: Per subject roi data. subject: time X ROI\n",
        "    :param method: method to use for correlation\n",
        "    :param output_dir: directory to save results\n",
        "    :param draw: Whether to draw the images or not\n",
        "    :param kwargs: kwargs passed to nilearn.plotting.plot_matrix\n",
        "    :return: list of correlation matrix for subjects.\n",
        "    \"\"\"\n",
        "    if '.' in fn_pattern:\n",
        "        raise ValueError(f'Invalid fn_pattern given: {fn_pattern}')\n",
        "\n",
        "    save_dir = os.path.join(output_dir, method)\n",
        "    if plot_dir:\n",
        "        plot_dir = os.path.join(plot_dir, method)\n",
        "\n",
        "    file_utils.InitDir(plot_dir)\n",
        "    file_utils.InitDir(save_dir)\n",
        "\n",
        "\n",
        "    # conn_m = ConnectivityMeasure(kind=method)\n",
        "    # data = [data.to_numpy() for data in subjects_data]\n",
        "    idx = 0\n",
        "    for ndata in tqdm(subjects_data, desc='Processing Slices', leave=True):\n",
        "        # conn_mats = conn_m.fit_transform([ndata])\n",
        "\n",
        "        # data = conn_mats[0]\n",
        "        # np.fill_diagonal(data, 0)\n",
        "        print(ndata.shape)\n",
        "        data = ndata.to_numpy()\n",
        "        # cdata = cupy.asarray(data)\n",
        "        # cmat = np.corrcoef(cdata, rowvar=False)\n",
        "        # np.fill_diagonal(cmat, 0)\n",
        "        mat = np.corrcoef(data, rowvar=False)\n",
        "        np.fill_diagonal(mat, 0)\n",
        "\n",
        "        logging.debug(f'Saving slice {idx}')\n",
        "        print(mat.shape)\n",
        "        # mat = cupy.asnumpy(cmat)\n",
        "        if draw and plot_dir:\n",
        "            plotting.plot_matrix(mat, **kwargs)\n",
        "            plt.savefig(\n",
        "                os.path.join(plot_dir, fn_pattern.format(idx=idx) + '.png')\n",
        "            )\n",
        "            plt.close()\n",
        "\n",
        "        # Save corr mat\n",
        "        np.save(\n",
        "            os.path.join(save_dir, fn_pattern.format(idx=idx) + '.csv'),\n",
        "            mat\n",
        "        )\n",
        "        idx += 1\n",
        "        return\n",
        "\n",
        "def CalculateAndPlotVoxelWiseDFCMatrices(\n",
        "        atlas: str, method: str, data: any,\n",
        "        output_dir: str,\n",
        "        plot_output_dir: str,\n",
        "        sliding_window_size: int,\n",
        "        force: bool = False,\n",
        "        standardize: bool = True,\n",
        "        plot_with_threshold: bool = False,\n",
        "        plot:bool = True,\n",
        "        network: Any = None\n",
        "):\n",
        "    logging.info(f'{atlas}, {method}')\n",
        "    sm_data = data[atlas][method]\n",
        "\n",
        "    for cond in tqdm(sm_data, desc='Calculating Voxel DFC'):\n",
        "        fc_dir = f'{output_dir}/{atlas}/{method}/correlation'\n",
        "        if not force and os.path.exists(f'{fc_dir}/{atlas}_{cond}_0.csv'):\n",
        "            continue\n",
        "\n",
        "        rois = sorted(sm_data[cond].keys(), key=lambda x: int(x))\n",
        "        cond_data = []\n",
        "        labels_range = 0\n",
        "        roi_voxel_mapping = {}\n",
        "        for roi in rois:\n",
        "            if network and roi not in network[1]:\n",
        "                continue\n",
        "\n",
        "            roi_data = sm_data[cond][roi]\n",
        "            if type(roi_data) is not pd.DataFrame:\n",
        "                raise ValueError(\n",
        "                    f'Voxel wise analysis need DataFrame, got {type(roi_data)}')\n",
        "\n",
        "            cond_data.append(roi_data)\n",
        "            if roi_data.isna().sum().sum() > 0:\n",
        "                print(roi, roi_data.isna().sum())\n",
        "                raise ValueError\n",
        "            roi_voxel_mapping[roi] = [i+labels_range for i in range(roi_data.shape[1])]\n",
        "            labels_range +=roi_data.shape[1]\n",
        "\n",
        "        df = pd.concat(cond_data, axis=1)\n",
        "        logging.debug('\\n', labels_range, df.shape)\n",
        "\n",
        "        if standardize:\n",
        "            df = df.apply(zscore, axis=1, ddof=1)\n",
        "\n",
        "        windows = []\n",
        "        for w_st in range(df.shape[0] - sliding_window_size):\n",
        "            windows.append(df.iloc[w_st:(w_st+sliding_window_size)])\n",
        "\n",
        "        vmin, vmax = -1, 1\n",
        "        if plot_with_threshold:\n",
        "            vmin, vmax = -0.8, 0.8\n",
        "        print(labels_range)\n",
        "        GetBasicCorrelationMatrixForSubjects(\n",
        "            subjects_data=windows,\n",
        "            method='correlation',\n",
        "            output_dir=f'{output_dir}/{atlas}/{method}/{cond}/{network[0]}',\n",
        "            plot_dir=f'{plot_output_dir}/{atlas}/{method}/{cond}/{network[0]}',\n",
        "            draw=plot,\n",
        "            figure=(20, 16),\n",
        "            labels=[i for i in range(labels_range)],\n",
        "            vmax=vmax, vmin=vmin,\n",
        "            title='CC, HP, M, WM_CSF',\n",
        "            reorder=True,\n",
        "            fn_pattern=f'{atlas}_{cond}_' + 'window_{idx}'\n",
        "        )\n",
        "\n",
        "def NetworkDFC():\n",
        "\n",
        "  root = f'/content/drive/MyDrive/Experiment/AWS_RESULTS/Results/{_EXP_LABEL}'\n",
        "\n",
        "  for atlas in atlases:\n",
        "    base = f'{root}/network/{atlas}'\n",
        "    metadata_fn = f'{base}/metadata.json'\n",
        "    metadata = json.load(open(metadata_fn))\n",
        "\n",
        "    for method in methods:\n",
        "      peak = metadata['avg']\n",
        "      communities_fn = (\n",
        "          f'{base}/avg/correlation/{peak[0]}/{peak[1]}_1.0/'\n",
        "          'greedy_communities.json')\n",
        "      communities = json.load(open(communities_fn))\n",
        "\n",
        "      community_map = {}\n",
        "      for idx, community in enumerate(communities):\n",
        "        community_map[idx] = community\n",
        "        CalculateAndPlotVoxelWiseDFCMatrices(\n",
        "            atlas, method, data=data,\n",
        "            output_dir=f'{root}/dynamic_functional_connectivity/voxel_wise',\n",
        "            plot_output_dir=f'{root}/plots/dynamic_functional_connectivity/voxel_wise',\n",
        "            sliding_window_size=10,\n",
        "            force=force,\n",
        "            plot=plot,\n",
        "            network=(idx, community))\n",
        "        return\n",
        "      community_map_fn = f'{root}/dynamic_functional_connectivity/voxel_wise/community_map.json'\n",
        "      json.dump(community_map, open(community_map_fn, 'w+'))\n",
        "\n",
        "if not skip_cell:\n",
        "  NetworkDFC()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65PcYMHiVCm8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class AnomalyDetector(Model):\n",
        "  def __init__(self):\n",
        "    super(AnomalyDetector, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Dense(64, activation=\"relu\"),\n",
        "      layers.Dense(32, activation=\"relu\"),\n",
        "      layers.Dense(16, activation=\"relu\"),\n",
        "      layers.Dense(8, activation=\"relu\")])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(16, activation=\"relu\"),\n",
        "      layers.Dense(32, activation=\"relu\"),\n",
        "      layers.Dense(64, activation=\"relu\"),\n",
        "      layers.Dense(8646, activation=\"sigmoid\")])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "autoencoder = AnomalyDetector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-CBTHRcWd3d"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ_aLLP1Wq0i"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import json\n",
        "# import scipy.cluster.hierarchy as shc\n",
        "# for method in ['avg', 'pca', 'pca_time', 'pct']:\n",
        "#   for cond in ['resting', 'rountine_run-1', 'rountine_run-2', 'rountine_run-3']:\n",
        "#     dfc_dir = '/content/drive/MyDrive/Experiment/AWS_RESULTS/Results/exp_bl/dynamic_functional_connectivity/harvard/{method}/{cond}/correlation'\n",
        "#     dl_dir = '/content/drive/MyDrive/Experiment/AWS_RESULTS/Results/exp_bl/deep_learning/harvard/{method}/{cond}/correlation'.format(method=method, cond=cond)\n",
        "#     file_utils.InitDir(dl_dir)\n",
        "    \n",
        "#     dfc_dir = dfc_dir.format(method=method, cond=cond)\n",
        "\n",
        "#     dfs = []\n",
        "#     for fn in os.listdir(dfc_dir):\n",
        "#       df = pd.read_csv(f'{dfc_dir}/{fn}',header=0, index_col=0)\n",
        "#       dfs.append(df)\n",
        "\n",
        "#     npdfs = [d.to_numpy()[np.triu_indices(132, 1)] for d in dfs]\n",
        "#     npdfs = np.array(npdfs)\n",
        "\n",
        "#     if cond  == 'resting':\n",
        "#       autoencoder.fit(npdfs[:150], npdfs[:150],\n",
        "#                   epochs=100,\n",
        "#                   shuffle=True,\n",
        "#                   validation_data=(npdfs[150:], npdfs[150:]))\n",
        "#     encoded_imgs = autoencoder.encoder(npdfs).numpy()\n",
        "#     decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
        "\n",
        "#     if cond == 'resting':\n",
        "#       plt.figure(figsize=(10, 7))  \n",
        "#       plt.title(f\"Dendrograms: {method}/{cond}\")  \n",
        "#       dend = shc.dendrogram(shc.linkage(encoded_imgs, method='ward'))\n",
        "#       plt.savefig(os.path.join(dl_dir, 'dendrogram.png'))\n",
        "\n",
        "#     from sklearn.cluster import AgglomerativeClustering\n",
        "#     cluster = AgglomerativeClustering(n_clusters=12, affinity='euclidean', linkage='ward')  \n",
        "#     result = cluster.fit_predict(encoded_imgs)\n",
        "\n",
        "#     encode_fn = os.path.join(dl_dir,'encoded.json')\n",
        "#     cluster_fn = os.path.join(dl_dir,'hierachical.json')\n",
        "\n",
        "#     json.dump(encoded_imgs.tolist(), open(encode_fn, 'w+'))\n",
        "#     json.dump(result.tolist(), open(cluster_fn, 'w+'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_25cxhfsqyB"
      },
      "outputs": [],
      "source": [
        "# for method in ['avg', 'pca', 'pca_time', 'pct']:\n",
        "#   for cond in ['resting', 'rountine_run-1', 'rountine_run-2', 'rountine_run-3']:\n",
        "#     dl_dir = '/content/drive/MyDrive/Experiment/AWS_RESULTS/Results/exp_bl/deep_learning/harvard/{method}/{cond}/correlation'.format(method=method, cond=cond)\n",
        "#     file_utils.InitDir(dl_dir)\n",
        "\n",
        "#     data = json.load(open(f'{dl_dir}/hierachical.json'))\n",
        "\n",
        "#     plt.figure(figsize=(10, 7))  \n",
        "#     plt.title(f\"Brain States Hisogram: {method}/{cond}\")\n",
        "#     plt.hist(data)\n",
        "#     plt.savefig(os.path.join(dl_dir, 'cluster_histogram.png'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umtZIJwsnZtE"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# for method in ['avg', 'pca', 'pca_time', 'pct']:\n",
        "#   for cond in ['resting', 'rountine_run-1', 'rountine_run-2', 'rountine_run-3']:\n",
        "#     dl_dir = '/content/drive/MyDrive/Experiment/AWS_RESULTS/Results/exp_bl/deep_learning/harvard/{method}/{cond}/correlation'.format(method=method, cond=cond)\n",
        "#     file_utils.InitDir(dl_dir)\n",
        "\n",
        "#     data = json.load(open(f'{dl_dir}/hierachical.json'))\n",
        "\n",
        "#     plt.figure(figsize=(10, 7))  \n",
        "#     plt.title(f\"Brain States Hisogram: {method}/{cond}\")\n",
        "#     plt.plot(range(len(data)), data, 'bo--')\n",
        "#     plt.savefig(os.path.join(dl_dir, 'cluster_line.png'))\n",
        "#     plt.show()\n",
        "#   break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xJNfGGJYi8p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "npdfs = [d.to_numpy()[np.triu_indices(132, 1)] for d in dfs]\n",
        "npdfs = np.array(npdfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnF2RQmQYrLJ"
      },
      "outputs": [],
      "source": [
        "npdfs = npdfs[..., tf.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOo5FAcSah52"
      },
      "outputs": [],
      "source": [
        "npdfs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA4ALtHDWhfQ"
      },
      "outputs": [],
      "source": [
        "autoencoder.fit(npdfs[:150], npdfs[:150],\n",
        "                epochs=100,\n",
        "                shuffle=True,\n",
        "                validation_data=(npdfs[150:], npdfs[150:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk0-Wt-aXwRT"
      },
      "outputs": [],
      "source": [
        "encoded_imgs = autoencoder.encoder(npdfs).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWgjWqzDkost"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster import hierarchy\n",
        "\n",
        "plt.figure(figsize=(100, 70))  \n",
        "plt.title(\"Dendrograms\")  \n",
        "dend = hierarchy.dendrogram(hierarchy.linkage(encoded_imgs, method='ward'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF3AueEklyja"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "cluster = AgglomerativeClustering(n_clusters=12, affinity='euclidean', linkage='ward')  \n",
        "result = cluster.fit_predict(encoded_imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxMtKWnAmcdQ"
      },
      "outputs": [],
      "source": [
        "pyplot.hist(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssh9GyuLl6LF"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pJZK9Yub3Dj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "size_X = 132\n",
        "for i in range(170):\n",
        "  X = np.zeros((size_X,size_X))\n",
        "  X[np.triu_indices(X.shape[0], k = 1)] = decoded_imgs[i]\n",
        "  X = X + X.T - np.diag(np.diag(X))\n",
        "  plt.imshow(X, cmap='hot', interpolation='nearest')\n",
        "  plt.show()\n",
        "  plt.imshow(dfs[i], cmap='hot', interpolation='nearest')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ke8zK9TcTWc"
      },
      "outputs": [],
      "source": [
        "plt.imshow(npdfs[0], cmap='hot', interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dazysT5Zb7_7"
      },
      "outputs": [],
      "source": [
        "\n",
        "aws_mgr2 = aws_utils.AWSManager(bucket='brainsurfer-fin')\n",
        "aws_mgr2.UploadDirectory(\n",
        "    dir=f'/content/drive/MyDrive/Experiment/AWS_RESULTS/Results/exp_bl/dynamic_functional_connectivity/harvard',\n",
        "    rm_prefix='/content/drive/MyDrive/Experiment/AWS_RESULTS/',\n",
        "    dry_run=False,\n",
        "    max_workers=50,\n",
        "    max_queue=50\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bZjQVajvaBAP"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNKLDSYK+XjiugNMp32gWNd",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}